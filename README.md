# Trabajos Pr谩cticos - Procesamiento del Lenguaje Natural (NLP)

**Especializaci贸n en Inteligencia Artificial - Universidad de Buenos Aires (UBA)**

Este repositorio contiene los cuatro desaf铆os desarrollados para la materia de Procesamiento del Lenguaje Natural de la Especializaci贸n en Inteligencia Artificial de la Universidad de Buenos Aires.

##  Contenido del Repositorio

###  [Desaf铆o 1: Vectorizaci贸n de Documentos y Clasificaci贸n](./desafio_1/)
**Archivo:** `Desafio_1.ipynb`

**Objetivos:**
- Vectorizaci贸n de documentos usando TF-IDF
- An谩lisis de similaridad coseno entre documentos
- Implementaci贸n de clasificaci贸n por prototipos (zero-shot)
- Entrenamiento de modelos Na茂ve Bayes (Multinomial y Complement)
- An谩lisis de similaridad entre palabras mediante matriz t茅rmino-documento

**Dataset:** 20 Newsgroups de sklearn

**T茅cnicas implementadas:**
- TF-IDF Vectorization
- Cosine Similarity
- Prototype-based Classification
- Multinomial Naive Bayes
- Complement Naive Bayes
- Document-term matrix transposition

**Resultados destacados:**
- Los documentos se agrupan coherentemente con su categor铆a tem谩tica
- La limpieza de texto mejora significativamente la representaci贸n sem谩ntica
- Las palabras relacionadas tem谩ticamente aparecen como vecinas en el espacio vectorial

###  [Desaf铆o 2: Word Embeddings con Gensim](./desafio_2/)
**Archivo:** `Desafio_2.ipynb`

**Objetivos:**
- Creaci贸n de embeddings personalizados con Word2Vec
- An谩lisis de relaciones sem谩nticas en el espacio de embeddings
- Visualizaci贸n de embeddings en 2D y 3D
- Interpretaci贸n de similitudes y diferencias entre palabras

**Dataset:** Corpus de letras de canciones de Eminem

**T茅cnicas implementadas:**
- Word2Vec (Skip-gram model)
- t-SNE para reducci贸n de dimensionalidad
- Visualizaci贸n interactiva con Plotly
- An谩lisis de similitud sem谩ntica

**Archivos de visualizaci贸n generados:**
- `embeddings_2d.html` - Visualizaci贸n 2D interactiva
- `embeddings_3d.html` - Visualizaci贸n 3D interactiva

**Hallazgos principales:**
- Los embeddings capturan el uso espec铆fico del vocabulario en el contexto musical
- Las palabras se agrupan seg煤n el estilo y contexto particular del artista
- La visualizaci贸n revela clusters tem谩ticos espec铆ficos del corpus

###  [Desaf铆o 3: Predicci贸n de Pr贸xima Palabra](./desafio_3/)
**Archivo:** `Desafio_3.ipynb`

**Objetivos:**
- Implementaci贸n de modelo de lenguaje para predicci贸n de secuencias
- Entrenamiento de redes LSTM para generaci贸n de texto
- Evaluaci贸n con m茅trica de perplejidad
- Implementaci贸n de beam search para generaci贸n

**Dataset:** Corpus de letras de canciones de Eminem

**Arquitectura del modelo:**
- Embedding Layer (dimensi贸n 5)
- 2 capas LSTM (64 unidades cada una) con Dropout
- Capa Dense para clasificaci贸n multiclase
- Funci贸n de p茅rdida: Sparse Categorical Crossentropy

**T茅cnicas implementadas:**
- Tokenizaci贸n y padding de secuencias
- Arquitectura Encoder-Decoder con LSTM
- Teacher forcing durante entrenamiento
- Beam search para generaci贸n de texto
- Interfaz interactiva con Gradio

**Resultados:**
- Accuracy de entrenamiento: ~30%
- Accuracy de validaci贸n: ~17%
- El modelo captura patrones b谩sicos pero requiere m谩s datos para mejor generalizaci贸n

###  [Desaf铆o 4: Traductor Secuencia a Secuencia](./desafio_4/)
**Archivo:** `Desafio_4.ipynb`

**Objetivos:**
- Replicaci贸n de modelo traductor en PyTorch
- Experimentaci贸n con diferentes arquitecturas LSTM
- Evaluaci贸n del impacto del tama帽o de capas recurrentes
- Implementaci贸n de embeddings pre-entrenados (GloVe)

**Dataset:** Corpus de traducci贸n Espa帽ol-Ingl茅s (Anki/ManyThings)

**Modelos implementados:**
1. **Modelo Base:** LSTM de 128 unidades, 1 capa
2. **Modelo Expandido:** LSTM de 256 unidades, 1 capa  
3. **Modelo Profundo:** LSTM de 64 unidades, 5 capas

**Arquitectura:**
- **Encoder:** Embedding + LSTM 
- **Decoder:** Embedding + LSTM + Dense + Softmax
- **Embeddings:** GloVe pre-entrenados (50 dimensiones)
- **Optimizador:** Adam con diferentes learning rates

**Resultados comparativos:**
- **LSTM 128 unidades:** Accuracy validation ~71.7%, mejor rendimiento general
- **LSTM 256 unidades:** Accuracy validation ~71.3%, comportamiento estable
- **LSTM 64 unidades (5 capas):** Accuracy validation ~63.9%, no mejora con mayor profundidad

**Conclusiones:**
- El modelo de 128 unidades mostr贸 el mejor balance rendimiento/estabilidad
- Mayor profundidad no necesariamente mejora el rendimiento
- Los embeddings pre-entrenados mejoran significativamente la representaci贸n

##  Tecnolog铆as Utilizadas

### Librer铆as principales:
- **scikit-learn** - Vectorizaci贸n, clasificaci贸n y m茅tricas
- **gensim** - Word embeddings y Word2Vec
- **tensorflow/keras** - Redes neuronales y procesamiento de secuencias
- **pytorch** - Implementaci贸n de modelos seq2seq
- **plotly** - Visualizaciones interactivas
- **gradio** - Interfaces de usuario interactivas
- **numpy, pandas** - Manipulaci贸n de datos
- **matplotlib, seaborn** - Visualizaciones est谩ticas

### Datasets utilizados:
- **20 Newsgroups** (sklearn) - Clasificaci贸n de documentos
- **Corpus Eminem** - Word embeddings y generaci贸n de texto
- **Espa帽ol-Ingl茅s Anki** - Traducci贸n autom谩tica

##  M茅tricas y Evaluaci贸n

- **F1-Score macro** para clasificaci贸n
- **Accuracy** para modelos de secuencia
- **Perplejidad** para modelos de lenguaje
- **Similaridad coseno** para an谩lisis de embeddings
- **Cross-entropy loss** para entrenamiento

##  Instrucciones de Uso

1. **Clonar el repositorio:**
```bash
git clone https://github.com/MaximilianoBernardo/ceia_nlp_tps.git
cd ceia_nlp_tps
```

2. **Instalar dependencias:**
```bash
pip install scikit-learn gensim tensorflow torch plotly gradio numpy pandas matplotlib seaborn
```

3. **Ejecutar notebooks:**
- Abrir cualquier archivo `.ipynb` en Jupyter Notebook/Lab
- Ejecutar las celdas secuencialmente
- Los datasets se descargan autom谩ticamente

##  Resultados Destacados

### Desaf铆o 1 - Clasificaci贸n de Textos
- **Na茂ve Bayes Multinomial:** Rendimiento superior en clasificaci贸n de noticias
- **Similaridad sem谩ntica:** Agrupaci贸n coherente de documentos por tem谩tica

### Desaf铆o 2 - Word Embeddings  
- **Visualizaci贸n 2D/3D:** Clusters tem谩ticos bien definidos
- **Relaciones sem谩nticas:** Captura del vocabulario espec铆fico del artista

### Desaf铆o 3 - Generaci贸n de Texto
- **Modelo LSTM:** Captura de patrones b谩sicos del lenguaje
- **Beam Search:** Generaci贸n m谩s diversa que greedy search

### Desaf铆o 4 - Traducci贸n Autom谩tica
- **Mejor configuraci贸n:** LSTM 128 unidades con 72% accuracy
- **Embeddings pre-entrenados:** Mejora significativa en representaci贸n

##  Contexto Acad茅mico

**Materia:** Procesamiento del Lenguaje Natural  
**Programa:** Especializaci贸n en Inteligencia Artificial  
**Universidad:** Universidad de Buenos Aires (UBA)  
**Autor:** Maximiliano Bernardo  

##  Licencia

Este proyecto est谩 bajo la Licencia Apache 2.0 - ver el archivo [LICENSE](LICENSE) para m谩s detalles.

##  Contribuciones

Este repositorio es parte de un proyecto acad茅mico. Las sugerencias y mejoras son bienvenidas a trav茅s de issues o pull requests.

---

*Desarrollado como parte de los trabajos pr谩cticos de la materia Procesamiento del Lenguaje Natural, Especializaci贸n en Inteligencia Artificial, Universidad de Buenos Aires.*